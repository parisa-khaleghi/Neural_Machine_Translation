# Attention-Based Neural Machine Translation

## Introduction
This repository implements the method from the paper "NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE" by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. The project enhances translation accuracy by incorporating an Attention layer in the Encoder-Decoder architecture, alongside a bidirectional RNN, using PyTorch.

## The project structure
* pytorch-seq2seq:
    >- en_tr_dataset
    >>- dataset_generator.py    
    >>- define_test_train_val_tr_en.py
    >>- en_tr_neural_machine_translation.ipynb
    >>- en_dataset.csv
    >>- tr_dataset.csv
    >- fr_en_dataset_in_article
    >>- define_test_train_val_fr_en.py
    >>- kaggle_neural_machine_translation.ipynb
    >>- local_neural_machine_translation.ipynb
    >- img    
    >- README.md

## Installation

### Requirements
- Python 3
- PyTorch
- SpaCy (older version for Turkish language support)

### Setup
To set up the project environment:
```bash
git clone [repository URL]
cd [repository name]
pip install -r requirements.txt
```

## Datasets

The implementation covers two datasets:
<ol>
<li>French to English: As provided in the original article.</li>
<li>English to Turkish: Generated using the tatoebatools library in Python.</li>
</ol>

## Model Architecture

The model includes:

- A bidirectional RNN for contextual information processing.
- An Attention mechanism for enhanced focus on relevant input sequence parts.

## Preparing Data
for the fr_en dataset in <a href="https://www.statmt.org/europarl/v7/fr-en.tgz">this link</a>:
```bash
`train_data, valid_data, test_data = Multi30k.splits(exts = ('.fr', '.en'), 
                                                    fields = (SRC, TRG))`    
```                                                    
The training data is multi30k.
The other dataset was generated by 'dataset_generator.py'

## Building the Seq2Seq Model

### Encoder
As we want our model to look back over the whole source sentence we return outputs, the stacked forward and backward hidden states for every token in the source sentence. We also return hidden, which acts as our initial hidden state in the decoder.

### Attention
Next up is the attention layer. This will take in the previous hidden state of the decoder

### Decoder
The decoder contains the attention layer, attention, which takes the previous hidden state, s_t-1, all of the encoder hidden states, H, and returns the attention vector, a_t.

We then use this attention vector to create a weighted source vector, w_t, denoted by weighted, which is a weighted sum of the encoder hidden states, H, using a_t as the weights.

The input word (that has been embedded),y_t, the weighted source vector, w_t, and the previous decoder hidden state, s_{t-1}, are then all passed into the decoder RNN, with $y_t$ and w_t being concatenated together.


We then pass y_t, w_t, and s_t through the linear layer, f, to predict the next word in the target sentence. This is done by concatenating them all together.

The image below shows decoding the first word in an example translation.

### Seq2Seq
This is the first model where we don't have to have the encoder RNN and decoder RNN have the same hidden dimensions, however the encoder has to be bidirectional. This requirement can be removed by changing all occurences of enc_dim * 2 to enc_dim * 2 if encoder_is_bidirectional else enc_dim.

This seq2seq encapsulator is similar to the last two. The only difference is that the encoder returns both the final hidden state (which is the final hidden state from both the forward and backward encoder RNNs passed through a linear layer) to be used as the initial hidden state for the decoder, as well as every hidden state (which are the forward and backward hidden states stacked on top of each other). We also need to ensure that hidden and encoder_outputs are passed to the decoder.

## Optimization and Results
- Optimizer: Used Adam optimizer with a learning rate of 0.01.
- Dropout: Implemented to balance underfitting and overfitting.
- The adjustments led to improved loss metrics and model generalization.
    
## Conclusion
1. This project implemented a Seq2Seq-based model for Attention translation tasks.
2. It used multi30k as the original data.
3. To decrease the loss, an Adam optimizer was used.
4. To prevent overfitting, dropout was used.

## Contributing

Contributions for improving the model or extending its language support are welcome. Please follow the standard pull request process.

## Citation

If this repository aids your research, consider citing the original work:
```bash
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
```
